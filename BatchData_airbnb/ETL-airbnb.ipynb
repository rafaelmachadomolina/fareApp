{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL to transform Airbnb rawdata to database schema\n",
    "\n",
    "This ETL is coded in pyspark. Though the current data size could be handled in Pandas or similar libraries, pyspark was chosen for scale reasons.\n",
    "\n",
    "The present ETL has two purposes: the main is to read and transform the obtained Airbnb files into a structured, table-like set of files useful for a normalised relational database, with incorporated schema. The second goal is to push the pyspark dataframes to the dedicated PostgreSQL database, as well as saving backups locally and in a dedicated AWS S3 bucket for this purpose.\n",
    "\n",
    "In a previous notebook, the files were explored to gain a better understanding of the data relations. Hence, from 6 CSV files, 4 are used in this ETL, obtaining a total of 8 tables. The following graph aims to plot the flow of data through the ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ETL data flow](images/ETL_Airbnb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "- [Initialisation](#initialisation)\n",
    "- [Functions](#functions)\n",
    "- [Neighbourhoods](#neighbourhoods)\n",
    "- [Listings](#listings)\n",
    "    - [Listings main](#listings_main)\n",
    "    - [Hosts main](#hosts)\n",
    "    - [Host verification](#verification)\n",
    "    - [Listing amenities](#amenities)\n",
    "    - [Listing complements](#complements)\n",
    "- [Reviews](#reviews)\n",
    "- [Calendar](#calendar)\n",
    "- [Push to S3](#s3)\n",
    "- [Push to database](#database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation <a id=initialisation></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session:\n",
      "<pyspark.sql.session.SparkSession object at 0x7f7288a5dbb0>\n"
     ]
    }
   ],
   "source": [
    "### Import modules\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, Window, SQLContext\n",
    "from pyspark.sql import functions as psF\n",
    "from pyspark.sql import types as psDT\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkWithPostgres')\\\n",
    "        .config(\"spark.driver.extraClassPath\", \"./postgresql-42.3.2.jar\")\\\n",
    "        .getOrCreate()\n",
    "print('Spark session:')\n",
    "print(spark)\n",
    "\n",
    "import pyspark\n",
    "# from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation of Spark context\n",
      "<SparkContext master=local[*] appName=SparkWithPostgres>\n",
      "\n",
      "Initialisation of SparkSQL\n",
      "<pyspark.sql.context.SQLContext object at 0x7f728837a670>\n"
     ]
    }
   ],
   "source": [
    "### Initialise Spark context\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    "print('Initialisation of Spark context')\n",
    "print(sc)\n",
    "print('')\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "print('Initialisation of SparkSQL')\n",
    "print(sqlContext)\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants\n",
    "\n",
    "CSV_PATH = './OriginalData_csv/'\n",
    "PARQUET_PATH = './SchemaReadyData_parquet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Database credentials\n",
    "\n",
    "DB_HOST = os.environ.get('DB_HOST')\n",
    "DB_PORT = os.environ.get('DB_PORT')\n",
    "DB_NAME = os.environ.get('DB_NAME')\n",
    "DB_USERNAME = os.environ.get('DB_USERNAME')\n",
    "DB_PASSWORD = os.environ.get('DB_PASSWORD')\n",
    "\n",
    "POSTGRES_URL = 'jdbc:postgresql://' + DB_HOST + ':' + DB_PORT + '/' + DB_NAME\n",
    "SCHEMA = 'listings.'\n",
    "PROPERTIES = {'user': DB_USERNAME,\n",
    "             'password': DB_PASSWORD,\n",
    "             'driver': 'org.postgresql.Driver'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions <a id=functions></a>\n",
    "\n",
    "Some processes are repeated throughout this ETL. In that spirit, functions are created to carry out recurrent tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition of functions\n",
    "\n",
    "# Functions to unfold arrays saved as strings into an array type of strings\n",
    "# Credit: https://silpara.medium.com/pyspark-string-to-array-of-string-in-dataframe-b9572233ccea\n",
    "def parse_array_from_string(x):\n",
    "    '''\n",
    "    Recieves a string and returns it into a json readable format in order to be\n",
    "    transformed to an array.\n",
    "    \n",
    "        params:\n",
    "            x (string): a single string\n",
    "            \n",
    "        returns:\n",
    "            res (list): a list (array) containing the strings\n",
    "    '''\n",
    "    res = json.loads(x)\n",
    "    return res;\n",
    "\n",
    "retrieve_array_func = psF.udf(parse_array_from_string, psDT.ArrayType(psDT.StringType()))\n",
    "\n",
    "# Read CSV file into a spark dataframe\n",
    "def read_csv(filename, path = CSV_PATH):\n",
    "    '''\n",
    "    This function reads a csv file from a given path. It takes into account multiline text fileds\n",
    "    inside the csv (containing quotation marks and characters like \\n, \\r)\n",
    "    \n",
    "        params:\n",
    "            filename (string): filename with extension of the csv file\n",
    "            path (string) OPTIONAL: path where the csv is located\n",
    "            \n",
    "        returns:\n",
    "            df (pyspark dataframe): data loaded from the file\n",
    "    '''\n",
    "    df = spark.read.options(delimiter = ',',\n",
    "                                 header = True,\n",
    "                                 #lineSep = '\\n',\n",
    "                                 escape = '\"',\n",
    "                                 multiline = True).csv(CSV_PATH + filename)\n",
    "    \n",
    "    # Show schema\n",
    "    print('Schema of dataframe in ' + filename)\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Return dataframe\n",
    "    return df;\n",
    "\n",
    "# Drop column(s) of dataframe\n",
    "def drop_columns(df, columns):\n",
    "    '''\n",
    "    Drops a column (or columns) from a given dataframe\n",
    "    \n",
    "        params:\n",
    "            df (pyspark dataframe): data with column(s) to be dropped\n",
    "            columns (string OR list): column(s) to be removed\n",
    "        \n",
    "        returns:\n",
    "            return_df (pyspark dataframe): dataframe with column(s) removed\n",
    "    '''\n",
    "    if(type(columns) == list):\n",
    "        df_return = df.drop(*columns)\n",
    "    else:\n",
    "        df_return = df.drop(columns)\n",
    "        \n",
    "    return df_return;\n",
    "\n",
    "# Trim whitespacing before and after string\n",
    "def trim_whitespace(df, column):\n",
    "    '''\n",
    "    Trims leading and trailing whitespaces in string fields\n",
    "    \n",
    "        params:\n",
    "            df (pyspark dataframe): dataframe containing strings\n",
    "            column (string): string representing column to be trimmed\n",
    "        \n",
    "        returns:\n",
    "            return_df (pyspark dataframe): dataframe with trimmed strings\n",
    "    '''\n",
    "    df_return = df.withColumn(column, psF.trim(column))\n",
    "    return df_return;\n",
    "\n",
    "# Add index to a given dataframe\n",
    "def add_index(df, index_name = 'id'):\n",
    "    '''\n",
    "    Adds a new index column with values starting at 1\n",
    "    \n",
    "        params:\n",
    "            df (pyspark dataframe): dataframe to add index to\n",
    "            index_name (string) OPTIONAL: name of new column\n",
    "        returns:\n",
    "            return_df (pyspark dataframe): dataframe with added index column\n",
    "    '''\n",
    "    df_return = df.withColumn(index_name,\n",
    "                              psF.row_number().over(Window.orderBy(psF.monotonically_increasing_id())))\n",
    "    df_return = df_return.withColumn(index_name,\n",
    "                                     df_return[index_name].cast(psDT.LongType()))\n",
    "    return df_return;\n",
    "\n",
    "# Rename column(s)\n",
    "def rename(df, col_in, col_out):\n",
    "    '''\n",
    "    Renames a column from the dataframe\n",
    "    \n",
    "        params:\n",
    "            df (pyspark dataframe): dataframe containing column to be renamed\n",
    "            col_in (string): name of original column\n",
    "            col_out (string): new name of column\n",
    "        \n",
    "        returns:\n",
    "            return_df (pyspark dataframe): dataframe with added index column\n",
    "    '''\n",
    "    df_return = df.withColumnRenamed(col_in, col_out)\n",
    "    return df_return;\n",
    "\n",
    "# Remove substring\n",
    "def remove_substr(df, column, substr):\n",
    "    '''\n",
    "    Function to eliminate a substring from a column. Useful to format strings\n",
    "    \n",
    "        params:\n",
    "            df (pyspark dataframe): dataframe containing column to formatted\n",
    "            column (string): name of column where the substring is to be removed\n",
    "            substr (string): string to be eliminated from column content\n",
    "        \n",
    "        returns:\n",
    "            return_df (pyspark dataframe): dataframe with formatted column\n",
    "    '''\n",
    "    df_return = df.withColumn(column, psF.regexp_replace(column, substr, ''))\n",
    "    return df_return;\n",
    "\n",
    "# Parse nulls\n",
    "def parse_nulls(df, column):\n",
    "    '''\n",
    "    Function to parse stings equal to null (None OR N/A) and convert them to NULL\n",
    "    \n",
    "        params:\n",
    "            df (pyspark dataframe): dataframe to be parsed\n",
    "            column (string): column containing string nulls\n",
    "        \n",
    "        returns:\n",
    "            return_df (pyspark dataframe): dataframe with formatted column containing\n",
    "                NULLs instead of None or N/A\n",
    "    '''\n",
    "    df_return = df.withColumn(column,\n",
    "                              psF.when((psF.col(column) == 'None') | (psF.col(column) == 'N/A'), None) \\\n",
    "                              .otherwise(psF.col(column)))\n",
    "    return df_return;\n",
    "\n",
    "# Cast column into datatpye\n",
    "def cast_col(df, column, dtype):\n",
    "    '''\n",
    "    Function to change the data type of a column\n",
    "    \n",
    "        params:\n",
    "            df (pyspark dataframe): dataframe containing column of interest\n",
    "            column (string): column subjected to new type\n",
    "            dtype (pyspark data type): data type from pySpark to be implemented\n",
    "        \n",
    "        returns:\n",
    "            return_df (pyspark dataframe): dataframe with changed data type in given column\n",
    "        \n",
    "        raises:\n",
    "            error: if attempted cast is not successful\n",
    "    '''\n",
    "    df_return = df.withColumn(column, df[column].cast(dtype))\n",
    "    return df_return;\n",
    "\n",
    "# Save dataframe to parquet\n",
    "def write_parquet(df, filename, path = PARQUET_PATH, OVERWRITE = True):\n",
    "    '''\n",
    "    Saves dataframe to parquet format in given path\n",
    "    \n",
    "        params:\n",
    "            df (pyspark dataframe): dataframe to be saved into parquet format\n",
    "            filename (string): name of target file, including extension (.parquet)\n",
    "            path (string) OPTIONAL: path to write the file\n",
    "            OVERWRITE (bool) OPTIONAL: if True, overwrites existing files. If not, function is not executed should\n",
    "                another file with the same name exists.\n",
    "    '''\n",
    "    if(OVERWRITE):\n",
    "        df.write.mode('overwrite').parquet(path + filename)\n",
    "    else:\n",
    "        df.write.parquet(path + filename)\n",
    "        \n",
    "# Save dataframe to parquet\n",
    "def insert_to_db(df,\n",
    "                 table_name,\n",
    "                 schema = SCHEMA,\n",
    "                 url = POSTGRES_URL,\n",
    "                 properties = PROPERTIES,\n",
    "                 overwrite = False,\n",
    "                 sample = False):\n",
    "    '''\n",
    "    Inserts contents of dataframe to PostgreSQL table\n",
    "    \n",
    "        params:\n",
    "            df (pyspark dataframe): dataframe to be inserted into database\n",
    "            table_name (string): name of target table. Must be created in db schema\n",
    "            schema (string) OPTIONAL: name of schema, default initiated with notebook\n",
    "            url (string) OPTIONAL: database host, port and database name in appropiate format\n",
    "            properties (dict) OPTIONAL: contains additional parameters to connect to db (username, password, driver)\n",
    "            overwrite (bool) OPTIONAL: if True, overwrites existing files. Default False\n",
    "            sample (bool) OPTIONAL: If True, inserts only 20 rows to database. Default False\n",
    "    '''\n",
    "    # Calculate sample if indicated\n",
    "    if(sample):\n",
    "        df = df.limit(20)\n",
    "        \n",
    "    # Add writing mode to properties if indicated\n",
    "    if(overwrite):\n",
    "        mode = 'overwrite'\n",
    "    else:\n",
    "        mode = 'append'\n",
    "        \n",
    "    # Execute insertion into the database\n",
    "    df.write.jdbc(url,\n",
    "                  table = schema + table_name,\n",
    "                  mode = mode,\n",
    "                  properties = properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbourhoods <a id=neighbourhoods></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of dataframe in neighbourhoods.csv\n",
      "root\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Load neighbourhoods to Spark dataframe\n",
    "\n",
    "df_neighbourhoods = read_csv('neighbourhoods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of table neighbourhood\n",
      "root\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Drop empty column and add index column\n",
    "\n",
    "df_neighbourhoods_process = drop_columns(df_neighbourhoods, 'neighbourhood_group')\n",
    "\n",
    "# Trim neighbourhood column\n",
    "df_neighbourhoods_process = trim_whitespace(df_neighbourhoods_process, 'neighbourhood')\n",
    "\n",
    "# Add index column\n",
    "df_neighbourhoods_process = add_index(df_neighbourhoods_process)\n",
    "\n",
    "# Print schema\n",
    "print('Schema of table neighbourhood')\n",
    "df_neighbourhoods_process.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export neighbourhoods to parquet\n",
    "\n",
    "write_parquet(df_neighbourhoods_process, 'neighbourhoods.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listings <a id=listings></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of dataframe in listings.csv\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- scrape_id: string (nullable = true)\n",
      " |-- last_scraped: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- neighborhood_overview: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- host_url: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- host_since: string (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_about: string (nullable = true)\n",
      " |-- host_response_time: string (nullable = true)\n",
      " |-- host_response_rate: string (nullable = true)\n",
      " |-- host_acceptance_rate: string (nullable = true)\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- host_thumbnail_url: string (nullable = true)\n",
      " |-- host_picture_url: string (nullable = true)\n",
      " |-- host_neighbourhood: string (nullable = true)\n",
      " |-- host_listings_count: string (nullable = true)\n",
      " |-- host_total_listings_count: string (nullable = true)\n",
      " |-- host_verifications: string (nullable = true)\n",
      " |-- host_has_profile_pic: string (nullable = true)\n",
      " |-- host_identity_verified: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- neighbourhood_group_cleansed: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: string (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- bathrooms_text: string (nullable = true)\n",
      " |-- bedrooms: string (nullable = true)\n",
      " |-- beds: string (nullable = true)\n",
      " |-- amenities: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- maximum_nights: string (nullable = true)\n",
      " |-- minimum_minimum_nights: string (nullable = true)\n",
      " |-- maximum_minimum_nights: string (nullable = true)\n",
      " |-- minimum_maximum_nights: string (nullable = true)\n",
      " |-- maximum_maximum_nights: string (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: string (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: string (nullable = true)\n",
      " |-- calendar_updated: string (nullable = true)\n",
      " |-- has_availability: string (nullable = true)\n",
      " |-- availability_30: string (nullable = true)\n",
      " |-- availability_60: string (nullable = true)\n",
      " |-- availability_90: string (nullable = true)\n",
      " |-- availability_365: string (nullable = true)\n",
      " |-- calendar_last_scraped: string (nullable = true)\n",
      " |-- number_of_reviews: string (nullable = true)\n",
      " |-- number_of_reviews_ltm: string (nullable = true)\n",
      " |-- number_of_reviews_l30d: string (nullable = true)\n",
      " |-- first_review: string (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- review_scores_rating: string (nullable = true)\n",
      " |-- review_scores_accuracy: string (nullable = true)\n",
      " |-- review_scores_cleanliness: string (nullable = true)\n",
      " |-- review_scores_checkin: string (nullable = true)\n",
      " |-- review_scores_communication: string (nullable = true)\n",
      " |-- review_scores_location: string (nullable = true)\n",
      " |-- review_scores_value: string (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- calculated_host_listings_count: string (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: string (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: string (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: string (nullable = true)\n",
      " |-- reviews_per_month: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Load listings to Spark dataframe\n",
    "\n",
    "df_listings = read_csv('listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop empty columns (for more insight, consult EDA notebook)\n",
    "\n",
    "COLUMNS_TO_DROP = ['neighbourhood_group_cleansed',\n",
    "                  'bathrooms',\n",
    "                  'calendar_updated',\n",
    "                  'license']\n",
    "\n",
    "df_listings_drop = drop_columns(df_listings, COLUMNS_TO_DROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns on original file: 70\n",
      "Columns after the split: 70\n"
     ]
    }
   ],
   "source": [
    "### Create 4 tables from listings df\n",
    "\n",
    "print('Columns on original file:', len(df_listings_drop.columns))\n",
    "\n",
    "COLS_LISTINGS = ['id',\n",
    "                 'host_id',\n",
    "                 'listing_url',\n",
    "                 'scrape_id',\n",
    "                 'last_scraped',\n",
    "                 'name',\n",
    "                 'description',\n",
    "                 'neighborhood_overview',\n",
    "                 'picture_url',\n",
    "                 'neighbourhood',\n",
    "                 'neighbourhood_cleansed',\n",
    "                 'latitude',\n",
    "                 'longitude',\n",
    "                 'property_type',\n",
    "                 'room_type',\n",
    "                 'accommodates',\n",
    "                 'bathrooms_text',\n",
    "                 'bedrooms',\n",
    "                 'beds',\n",
    "                 'price',\n",
    "                 'minimum_nights',\n",
    "                 'maximum_nights']\n",
    "\n",
    "COLS_LISTINGS_AMENITIES = ['id',\n",
    "                           'amenities']\n",
    "\n",
    "COLS_HOSTS = ['host_id',\n",
    "              'host_url',\n",
    "              'host_name',\n",
    "              'host_since',\n",
    "              'host_location',\n",
    "              'host_about',\n",
    "              'host_response_time',\n",
    "              'host_response_rate',\n",
    "              'host_acceptance_rate',\n",
    "              'host_is_superhost',\n",
    "              'host_thumbnail_url',\n",
    "              'host_picture_url',\n",
    "              'host_neighbourhood',\n",
    "              'host_listings_count',\n",
    "              'host_total_listings_count',\n",
    "              'host_verifications',\n",
    "              'host_has_profile_pic',\n",
    "              'host_identity_verified']\n",
    "\n",
    "COLS_COMPLENET = ['id',\n",
    "                  'minimum_minimum_nights',\n",
    "                  'maximum_minimum_nights',\n",
    "                  'minimum_maximum_nights',\n",
    "                  'maximum_maximum_nights',\n",
    "                  'minimum_nights_avg_ntm',\n",
    "                  'maximum_nights_avg_ntm',\n",
    "                  'has_availability',\n",
    "                  'availability_30',\n",
    "                  'availability_60',\n",
    "                  'availability_90',\n",
    "                  'availability_365',\n",
    "                  'calendar_last_scraped',\n",
    "                  'number_of_reviews',\n",
    "                  'number_of_reviews_ltm',\n",
    "                  'number_of_reviews_l30d',\n",
    "                  'first_review',\n",
    "                  'last_review',\n",
    "                  'review_scores_rating',\n",
    "                  'review_scores_accuracy',\n",
    "                  'review_scores_cleanliness',\n",
    "                  'review_scores_checkin',\n",
    "                  'review_scores_communication',\n",
    "                  'review_scores_location',\n",
    "                  'review_scores_value',\n",
    "                  'instant_bookable',\n",
    "                  'calculated_host_listings_count',\n",
    "                  'calculated_host_listings_count_entire_homes',\n",
    "                  'calculated_host_listings_count_private_rooms',\n",
    "                  'calculated_host_listings_count_shared_rooms',\n",
    "                  'reviews_per_month']\n",
    "\n",
    "print('Columns after the split:',\n",
    "      len(COLS_LISTINGS) + len(COLS_HOSTS) + len(COLS_COMPLENET) + len(COLS_LISTINGS_AMENITIES) - 3) #Correction for 3 additioanl id columns\n",
    "\n",
    "# Create dataframes\n",
    "df_listings_main = df_listings_drop.select(COLS_LISTINGS)\n",
    "df_listings_amenities = df_listings_drop.select(COLS_LISTINGS_AMENITIES)\n",
    "df_hosts = df_listings_drop.select(COLS_HOSTS)\n",
    "df_listings_complements = df_listings_drop.select(COLS_COMPLENET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process listings main <a id=listings_main></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge with neighbourhoods to obtain neighbourhood_id\n",
    "\n",
    "df_neighbourhoods_merge = rename(df_neighbourhoods_process, 'id', 'neighbourhood_id')\n",
    "df_neighbourhoods_merge = rename(df_neighbourhoods_merge, 'neighbourhood', 'neighbourhood_cleansed')\n",
    "\n",
    "df_listings_merged = df_listings_main.join(df_neighbourhoods_merge,\n",
    "                                         how = 'left',\n",
    "                                         on = ['neighbourhood_cleansed'])\n",
    "\n",
    "df_listings_merged = drop_columns(df_listings_merged, 'neighbourhood_cleansed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of main listings\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- scrape_id: string (nullable = true)\n",
      " |-- last_scraped: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- neighborhood_overview: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: string (nullable = true)\n",
      " |-- bathrooms_text: string (nullable = true)\n",
      " |-- bedrooms: string (nullable = true)\n",
      " |-- beds: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- maximum_nights: string (nullable = true)\n",
      " |-- neighbourhood_id: long (nullable = true)\n",
      "\n",
      "Schema of table listings\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- host_id: long (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- scrape_id: long (nullable = true)\n",
      " |-- date_last_scraped: date (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- neighborhood_overview: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- neighbourhood_typed: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: integer (nullable = true)\n",
      " |-- bathrooms_text: string (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      " |-- neighbourhood_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Give structure to listings table\n",
    "\n",
    "print('Schema of main listings')\n",
    "df_listings_merged.printSchema()\n",
    "\n",
    "# Define dict with datatypes\n",
    "LISTINGS_MAIN_DTYPES = {'id': psDT.LongType(),\n",
    "                        'host_id': psDT.LongType(),\n",
    "                        'scrape_id': psDT.LongType(),\n",
    "                        'last_scraped': psDT.DateType(),\n",
    "                        'latitude': psDT.DoubleType(),\n",
    "                        'longitude': psDT.DoubleType(),\n",
    "                        'accommodates': psDT.IntegerType(),\n",
    "                        'bedrooms': psDT.IntegerType(),\n",
    "                        'beds': psDT.IntegerType(),\n",
    "                        'price': psDT.DoubleType(),\n",
    "                        'minimum_nights': psDT.IntegerType(),\n",
    "                        'maximum_nights': psDT.IntegerType(),\n",
    "                        'neighbourhood_id': psDT.IntegerType()}\n",
    "\n",
    "# Define dict with new names\n",
    "LISTINGS_MAIN_NAMES = {'last_scraped': 'date_last_scraped',\n",
    "                      'neighbourhood': 'neighbourhood_typed'}\n",
    "\n",
    "# Copy dataframe\n",
    "df_listings_main_process = df_listings_merged.select('*')\n",
    "\n",
    "# Drop $ sign from price\n",
    "df_listings_main_process = remove_substr(df_listings_main_process, 'price', '\\$')\n",
    "\n",
    "# Use stripping for strings\n",
    "for COLUMN in df_listings_main_process.columns:\n",
    "    \n",
    "    # Strip whitespaces\n",
    "    df_listings_main_process = trim_whitespace(df_listings_main_process, COLUMN)\n",
    "    \n",
    "    # Replace None and N/A with null\n",
    "    df_listings_main_process = parse_nulls(df_listings_main_process, COLUMN)\n",
    "    \n",
    "    # Change data type\n",
    "    if(COLUMN in LISTINGS_MAIN_DTYPES.keys()):\n",
    "        df_listings_main_process = cast_col(df_listings_main_process, COLUMN, LISTINGS_MAIN_DTYPES[COLUMN])\n",
    "    \n",
    "    # Rename columns\n",
    "    if(COLUMN in LISTINGS_MAIN_NAMES.keys()):\n",
    "        df_listings_main_process = rename(df_listings_main_process, COLUMN, LISTINGS_MAIN_NAMES[COLUMN])\n",
    "        \n",
    "# Print schema\n",
    "print('Schema of table listings')\n",
    "df_listings_main_process.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export listings to parquet\n",
    "\n",
    "write_parquet(df_listings_main_process, 'listings.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process hosts - main data <a id=hosts></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows on previous dataframe: 66641\n",
      "Rows on unique dataframe: 44695\n"
     ]
    }
   ],
   "source": [
    "### Drop duplicates\n",
    "\n",
    "df_hosts_unique = df_hosts.dropDuplicates()\n",
    "\n",
    "print('Rows on previous dataframe:', df_hosts.count())\n",
    "print('Rows on unique dataframe:', df_hosts_unique.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Separate host verifications\n",
    "\n",
    "df_host_verifications = df_hosts_unique.select(['host_id', 'host_verifications'])\n",
    "df_hosts_sliced = drop_columns(df_hosts_unique, 'host_verifications')\n",
    "\n",
    "# Replace None with null and drop those records (it means host has no verifications at all)\n",
    "df_host_verifications = parse_nulls(df_host_verifications, 'host_verifications')\n",
    "df_host_verifications = df_host_verifications.na.drop(subset = ['host_verifications'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of hosts sliced\n",
      "root\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- host_url: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- host_since: string (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_about: string (nullable = true)\n",
      " |-- host_response_time: string (nullable = true)\n",
      " |-- host_response_rate: string (nullable = true)\n",
      " |-- host_acceptance_rate: string (nullable = true)\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- host_thumbnail_url: string (nullable = true)\n",
      " |-- host_picture_url: string (nullable = true)\n",
      " |-- host_neighbourhood: string (nullable = true)\n",
      " |-- host_listings_count: string (nullable = true)\n",
      " |-- host_total_listings_count: string (nullable = true)\n",
      " |-- host_has_profile_pic: string (nullable = true)\n",
      " |-- host_identity_verified: string (nullable = true)\n",
      "\n",
      "Schema of table hosts\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- since: date (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- about: string (nullable = true)\n",
      " |-- response_time: string (nullable = true)\n",
      " |-- response_rate: double (nullable = true)\n",
      " |-- acceptance_rate: double (nullable = true)\n",
      " |-- is_superhost: boolean (nullable = true)\n",
      " |-- thumbnail_url: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- listings_count: integer (nullable = true)\n",
      " |-- total_listings_count: integer (nullable = true)\n",
      " |-- has_profile_pic: boolean (nullable = true)\n",
      " |-- identity_verified: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Give structure to hosts sliced table\n",
    "\n",
    "print('Schema of hosts sliced')\n",
    "df_hosts_sliced.printSchema()\n",
    "\n",
    "# Define dict with datatypes\n",
    "HOST_DTYPES = {'host_id': psDT.LongType(),\n",
    "               'host_since': psDT.DateType(),\n",
    "               'host_response_rate': psDT.DoubleType(),\n",
    "               'host_acceptance_rate': psDT.DoubleType(),\n",
    "               'host_is_superhost': psDT.BooleanType(),\n",
    "               'host_listings_count': psDT.IntegerType(),\n",
    "               'host_total_listings_count': psDT.IntegerType(),\n",
    "               'host_has_profile_pic': psDT.BooleanType(),\n",
    "               'host_identity_verified': psDT.BooleanType()}\n",
    "\n",
    "# Define features to be transformed to decimal from percentage\n",
    "PERCENTAGE_FEAT = ['host_response_rate', 'host_acceptance_rate']\n",
    "\n",
    "# Copy dataframe\n",
    "df_hosts_process = df_hosts_sliced.select('*')\n",
    "\n",
    "# Drop % sign from percentage feats\n",
    "for COLUMN in PERCENTAGE_FEAT:\n",
    "    df_hosts_process = remove_substr(df_hosts_process, COLUMN, '\\%')\n",
    "\n",
    "# Use stripping for strings\n",
    "for COLUMN in df_hosts_process.columns:\n",
    "    \n",
    "    # Strip whitespaces\n",
    "    df_hosts_process = trim_whitespace(df_hosts_process, COLUMN)\n",
    "    \n",
    "    # Replace None and N/A with null\n",
    "    df_hosts_process = parse_nulls(df_hosts_process, COLUMN)\n",
    "    \n",
    "    # Change data type\n",
    "    if(COLUMN in HOST_DTYPES.keys()):\n",
    "        df_hosts_process = cast_col(df_hosts_process, COLUMN, HOST_DTYPES[COLUMN])\n",
    "        \n",
    "    # Make percentage columns decimal\n",
    "    if(COLUMN in PERCENTAGE_FEAT):\n",
    "        df_hosts_process = df_hosts_process.withColumn(COLUMN,\n",
    "                                                   psF.col(COLUMN) / 100)\n",
    "    \n",
    "    # Rename columns by removing host_\n",
    "    df_hosts_process = rename(df_hosts_process, COLUMN, COLUMN[5:])\n",
    "        \n",
    "# Print schema\n",
    "print('Schema of table hosts')\n",
    "df_hosts_process.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export hosts to parquet\n",
    "\n",
    "write_parquet(df_hosts_process, 'hosts.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process hosts - verifications <a id=verification></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of hosts verifications\n",
      "root\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- host_verifications: string (nullable = true)\n",
      "\n",
      "Schema of table host verification\n",
      "root\n",
      " |-- host_id: long (nullable = true)\n",
      " |-- verification: string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Explode verifications into different rows for final table\n",
    "\n",
    "print('Schema of hosts verifications')\n",
    "df_host_verifications.printSchema()\n",
    "\n",
    "# Copy dataframe\n",
    "df_host_verifications_process = df_host_verifications.select('*')\n",
    "\n",
    "# Convert quotation to double quotation --> in order to properly load to json\n",
    "df_host_verifications_process = df_host_verifications_process.withColumn('host_verifications',\n",
    "                                                   psF.regexp_replace('host_verifications', '\\'', '\\\"'))\n",
    "\n",
    "# Change data types\n",
    "df_host_verifications_process = cast_col(df_host_verifications_process, 'host_id', psDT.LongType())\n",
    "df_host_verifications_process = df_host_verifications_process.withColumn('host_verifications',\n",
    "                                                       retrieve_array_func(psF.col('host_verifications')))\n",
    "\n",
    "# Explode list into rows\n",
    "df_host_verifications_process = df_host_verifications_process.select('host_id',\n",
    "                                                                    psF.explode(df_host_verifications_process.host_verifications) \\\n",
    "                                                                    .alias('verification'))\n",
    "\n",
    "# Add index column\n",
    "df_host_verifications_process = add_index(df_host_verifications_process)\n",
    "\n",
    "# Trim values of string\n",
    "df_host_verifications_process = trim_whitespace(df_host_verifications_process, 'verification')\n",
    "\n",
    "# Print schema\n",
    "print('Schema of table host verification')\n",
    "df_host_verifications_process.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export host verification to parquet\n",
    "\n",
    "write_parquet(df_host_verifications_process, 'host_verification.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process listings amenities <a id=amenities></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of amenities\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- amenities: string (nullable = true)\n",
      "\n",
      "Schema of table listing amenities\n",
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- amenity: string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Explode verifications into different rows for final table\n",
    "\n",
    "print('Schema of amenities')\n",
    "df_listings_amenities.printSchema()\n",
    "\n",
    "# Copy dataframe\n",
    "df_amenities_process = df_listings_amenities.select('*')\n",
    "\n",
    "# Rename id columns to listing_id\n",
    "df_amenities_process = rename(df_amenities_process, 'id', 'listing_id')\n",
    "\n",
    "# Change data types\n",
    "df_amenities_process = cast_col(df_amenities_process, 'listing_id', psDT.LongType())\n",
    "df_amenities_process = df_amenities_process.withColumn('amenities',\n",
    "                                                       retrieve_array_func(psF.col('amenities')))\n",
    "\n",
    "# Explode list into rows\n",
    "df_amenities_process = df_amenities_process.select('listing_id',\n",
    "                                                   psF.explode(df_amenities_process.amenities) \\\n",
    "                                                   .alias('amenity'))\n",
    "\n",
    "# Add index column\n",
    "df_amenities_process = add_index(df_amenities_process)\n",
    "\n",
    "# Trim values of string\n",
    "df_amenities_process = trim_whitespace(df_amenities_process, 'amenity')\n",
    "\n",
    "# Print schema\n",
    "print('Schema of table listing amenities')\n",
    "df_amenities_process.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export listing amenities to parquet\n",
    "\n",
    "write_parquet(df_amenities_process, 'listing_amenities.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process listings complements <a id=complements></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of listings complements\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- minimum_minimum_nights: string (nullable = true)\n",
      " |-- maximum_minimum_nights: string (nullable = true)\n",
      " |-- minimum_maximum_nights: string (nullable = true)\n",
      " |-- maximum_maximum_nights: string (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: string (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: string (nullable = true)\n",
      " |-- has_availability: string (nullable = true)\n",
      " |-- availability_30: string (nullable = true)\n",
      " |-- availability_60: string (nullable = true)\n",
      " |-- availability_90: string (nullable = true)\n",
      " |-- availability_365: string (nullable = true)\n",
      " |-- calendar_last_scraped: string (nullable = true)\n",
      " |-- number_of_reviews: string (nullable = true)\n",
      " |-- number_of_reviews_ltm: string (nullable = true)\n",
      " |-- number_of_reviews_l30d: string (nullable = true)\n",
      " |-- first_review: string (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- review_scores_rating: string (nullable = true)\n",
      " |-- review_scores_accuracy: string (nullable = true)\n",
      " |-- review_scores_cleanliness: string (nullable = true)\n",
      " |-- review_scores_checkin: string (nullable = true)\n",
      " |-- review_scores_communication: string (nullable = true)\n",
      " |-- review_scores_location: string (nullable = true)\n",
      " |-- review_scores_value: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- calculated_host_listings_count: string (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: string (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: string (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: string (nullable = true)\n",
      " |-- reviews_per_month: string (nullable = true)\n",
      "\n",
      "Schema of table listings complements\n",
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- minimum_minimum_nights: integer (nullable = true)\n",
      " |-- maximum_minimum_nights: integer (nullable = true)\n",
      " |-- minimum_maximum_nights: integer (nullable = true)\n",
      " |-- maximum_maximum_nights: integer (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: double (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: double (nullable = true)\n",
      " |-- has_availability: boolean (nullable = true)\n",
      " |-- availability_30: integer (nullable = true)\n",
      " |-- availability_60: integer (nullable = true)\n",
      " |-- availability_90: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- calendar_last_scraped: date (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- number_of_reviews_l30d: integer (nullable = true)\n",
      " |-- first_review: date (nullable = true)\n",
      " |-- last_review: date (nullable = true)\n",
      " |-- review_scores_rating: double (nullable = true)\n",
      " |-- review_scores_accuracy: double (nullable = true)\n",
      " |-- review_scores_cleanliness: double (nullable = true)\n",
      " |-- review_scores_checkin: double (nullable = true)\n",
      " |-- review_scores_communication: double (nullable = true)\n",
      " |-- review_scores_location: double (nullable = true)\n",
      " |-- review_scores_value: double (nullable = true)\n",
      " |-- instant_bookable: boolean (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: integer (nullable = true)\n",
      " |-- reviews_per_month: double (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Give structure to listings table\n",
    "\n",
    "print('Schema of listings complements')\n",
    "df_listings_complements.printSchema()\n",
    "\n",
    "# Define dict with datatypes\n",
    "COMPLEMENTS_MAIN_DTPYES = {'listing_id': psDT.LongType(),\n",
    "                          'minimum_minimum_nights': psDT.IntegerType(),\n",
    "                          'maximum_minimum_nights': psDT.IntegerType(),\n",
    "                          'minimum_maximum_nights': psDT.IntegerType(),\n",
    "                          'maximum_maximum_nights': psDT.IntegerType(),\n",
    "                          'minimum_nights_avg_ntm': psDT.DoubleType(),\n",
    "                          'maximum_nights_avg_ntm': psDT.DoubleType(),\n",
    "                          'has_availability': psDT.BooleanType(),\n",
    "                          'availability_30': psDT.IntegerType(),\n",
    "                          'availability_60': psDT.IntegerType(),\n",
    "                          'availability_90': psDT.IntegerType(),\n",
    "                          'availability_365': psDT.IntegerType(),\n",
    "                          'calendar_last_scraped': psDT.DateType(),\n",
    "                          'number_of_reviews': psDT.IntegerType(),\n",
    "                          'number_of_reviews_ltm': psDT.IntegerType(),\n",
    "                          'number_of_reviews_l30d': psDT.IntegerType(),\n",
    "                          'first_review': psDT.DateType(),\n",
    "                          'last_review': psDT.DateType(),\n",
    "                          'review_scores_rating': psDT.DoubleType(),\n",
    "                          'review_scores_accuracy': psDT.DoubleType(),\n",
    "                          'review_scores_cleanliness': psDT.DoubleType(),\n",
    "                          'review_scores_checkin': psDT.DoubleType(),\n",
    "                          'review_scores_communication': psDT.DoubleType(),\n",
    "                          'review_scores_location': psDT.DoubleType(),\n",
    "                          'review_scores_value': psDT.DoubleType(),\n",
    "                          'instant_bookable': psDT.BooleanType(),\n",
    "                          'calculated_host_listings_count': psDT.IntegerType(),\n",
    "                          'calculated_host_listings_count_entire_homes': psDT.IntegerType(),\n",
    "                          'calculated_host_listings_count_private_rooms': psDT.IntegerType(),\n",
    "                          'calculated_host_listings_count_shared_rooms': psDT.IntegerType(),\n",
    "                          'reviews_per_month': psDT.DoubleType()}\n",
    "\n",
    "# Copy dataframe\n",
    "df_listings_complements_process = df_listings_complements.select('*')\n",
    "\n",
    "# Rename id columns to listing_id\n",
    "df_listings_complements_process = rename(df_listings_complements_process, 'id', 'listing_id')\n",
    "\n",
    "# Transformations per column\n",
    "for COLUMN in df_listings_complements_process.columns:\n",
    "    \n",
    "    # Strip whitespaces\n",
    "    df_listings_complements_process = trim_whitespace(df_listings_complements_process, COLUMN)\n",
    "    \n",
    "    # Replace None and N/A with null\n",
    "    df_listings_complements_process = parse_nulls(df_listings_complements_process, COLUMN)\n",
    "    \n",
    "    # Change data type\n",
    "    if(COLUMN in COMPLEMENTS_MAIN_DTPYES.keys()):\n",
    "        df_listings_complements_process = cast_col(df_listings_complements_process,\n",
    "                                                   COLUMN,\n",
    "                                                   COMPLEMENTS_MAIN_DTPYES[COLUMN])\n",
    "\n",
    "# Add index column\n",
    "df_listings_complements_process = add_index(df_listings_complements_process)\n",
    "        \n",
    "# Print schema\n",
    "print('Schema of table listings complements')\n",
    "df_listings_complements_process.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export listing complements to parquet\n",
    "\n",
    "write_parquet(df_listings_complements_process, 'listing_complements.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews <a id=reviews></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of dataframe in reviews.csv\n",
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- reviewer_id: string (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Load reviews to Spark dataframe\n",
    "\n",
    "df_reviews = read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of reviews\n",
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- reviewer_id: string (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n",
      "Schema of table listings complements\n",
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- reviewer_id: long (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Give structure to reviews table\n",
    "\n",
    "print('Schema of reviews')\n",
    "df_reviews.printSchema()\n",
    "\n",
    "# Define dict with datatypes\n",
    "REVIEWS_MAIN_DTPYES = {'listing_id': psDT.LongType(),\n",
    "                      'id': psDT.LongType(),\n",
    "                      'date': psDT.DateType(),\n",
    "                      'reviewer_id': psDT.LongType()}\n",
    "\n",
    "# Copy dataframe\n",
    "df_reviews_process = df_reviews.select('*')\n",
    "\n",
    "# Transformations per column\n",
    "for COLUMN in df_reviews_process.columns:\n",
    "    \n",
    "    # Strip whitespaces\n",
    "    df_reviews_process = trim_whitespace(df_reviews_process, COLUMN)\n",
    "    \n",
    "    # Replace None and N/A with null\n",
    "    df_reviews_process = parse_nulls(df_reviews_process, COLUMN)\n",
    "    \n",
    "    # Change data type\n",
    "    if(COLUMN in REVIEWS_MAIN_DTPYES.keys()):\n",
    "        df_reviews_process = cast_col(df_reviews_process, COLUMN, REVIEWS_MAIN_DTPYES[COLUMN])\n",
    "    \n",
    "# Print schema\n",
    "print('Schema of table listings complements')\n",
    "df_reviews_process.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export reviews to parquet\n",
    "\n",
    "write_parquet(df_reviews_process, 'reviews.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calendar <a id=calendar></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of dataframe in calendar.csv\n",
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- available: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- adjusted_price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- maximum_nights: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Load reviews to Spark dataframe\n",
    "\n",
    "df_calendar = read_csv('calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of calendar\n",
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- available: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- adjusted_price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- maximum_nights: string (nullable = true)\n",
      "\n",
      "Schema of table calendar\n",
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- available: boolean (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- adjusted_price: double (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Give structure to calendar\n",
    "\n",
    "print('Schema of calendar')\n",
    "df_calendar.printSchema()\n",
    "\n",
    "# Define dict with datatypes\n",
    "CALENDAR_MAIN_DTPYES = {'listing_id': psDT.LongType(),\n",
    "                       'date': psDT.DateType(),\n",
    "                       'available': psDT.BooleanType(),\n",
    "                       'price': psDT.DoubleType(),\n",
    "                       'adjusted_price': psDT.DoubleType(),\n",
    "                       'minimum_nights': psDT.IntegerType(),\n",
    "                       'maximum_nights': psDT.IntegerType()}\n",
    "\n",
    "# Copy dataframe\n",
    "df_calendar_process = df_calendar.select('*')\n",
    "\n",
    "# Drop $ sign from price and adjusted_price\n",
    "df_calendar_process = remove_substr(df_calendar_process, 'price', '\\$')\n",
    "df_calendar_process = remove_substr(df_calendar_process, 'adjusted_price', '\\$')\n",
    "\n",
    "# # Transformations per column\n",
    "for COLUMN in df_calendar_process.columns:\n",
    "    \n",
    "    # Strip whitespaces\n",
    "    df_calendar_process = trim_whitespace(df_calendar_process, COLUMN)\n",
    "    \n",
    "    # Replace None and N/A with null\n",
    "    df_calendar_process = parse_nulls(df_calendar_process, COLUMN)\n",
    "    \n",
    "    # Change data type\n",
    "    if(COLUMN in CALENDAR_MAIN_DTPYES.keys()):\n",
    "        df_calendar_process = cast_col(df_calendar_process, COLUMN, CALENDAR_MAIN_DTPYES[COLUMN])\n",
    "\n",
    "# Add index column\n",
    "df_calendar_process = add_index(df_calendar_process)\n",
    "        \n",
    "# Print schema\n",
    "print('Schema of table calendar')\n",
    "df_calendar_process.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export calendar to parquet\n",
    "\n",
    "write_parquet(df_calendar_process, 'calendar.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push data to S3 bucket <a id=s3></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialise AWS params\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_parquet_bucket_name = os.environ.get('AWS_BUCKET_PARQUET')\n",
    "s3_csv_bucket_name = os.environ.get('AWS_BUCKET_CSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted previous versions\n"
     ]
    }
   ],
   "source": [
    "### Delete previous parquet files in bucket\n",
    "\n",
    "try:\n",
    "    objects_parquet = [object_.size for object_ in boto3.resource('s3').Bucket(s3_parquet_bucket_name).objects.all()]\n",
    "    if(len(objects_parquet) > 0):\n",
    "        s3 = boto3.resource('s3')\n",
    "        s3_parquet_bucket = s3.Bucket(s3_parquet_bucket_name)\n",
    "        s3_parquet_bucket.objects.all().delete()\n",
    "        print('Successfully deleted previous versions')\n",
    "    else:\n",
    "        print('Bucket was empty')\n",
    "except:\n",
    "    print('Something went wrong with this operation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write CSV files into S3 storage\n",
    "\n",
    "files = [f for f in os.listdir(CSV_PATH) if os.path.isfile(os.path.join(CSV_PATH, f))]\n",
    "for f in files:\n",
    "    s3_client.upload_file(CSV_PATH + f, s3_csv_bucket_name, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved copies in S3 bucket\n"
     ]
    }
   ],
   "source": [
    "### Write parquet files into S3 storage\n",
    "\n",
    "dir_parquet = [d for d in os.listdir(PARQUET_PATH) if os.path.isdir(os.path.join(PARQUET_PATH, d))]\n",
    "\n",
    "try:\n",
    "    for dirs in dir_parquet:\n",
    "        INNER_PATH = PARQUET_PATH + dirs\n",
    "        files = [f for f in os.listdir(INNER_PATH) if os.path.isfile(os.path.join(INNER_PATH, f))]\n",
    "\n",
    "        for f in files:\n",
    "            s3_client.upload_file(INNER_PATH + '/' + f,\n",
    "                                  s3_parquet_bucket_name,\n",
    "                                  dirs + '/' + f)\n",
    "    \n",
    "    print('Successfully saved copies in S3 bucket')\n",
    "\n",
    "except:\n",
    "    print('Oh oh, something went wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of csv backup: 1488.2 MB\n",
      "Total size of parquet backup: 338.8 MB\n",
      "Compression ratio: 4.39\n"
     ]
    }
   ],
   "source": [
    "### Print total size of buckets\n",
    "\n",
    "object_size_csv = [object_.size for object_ in boto3.resource('s3').Bucket(s3_csv_bucket_name).objects.all()]\n",
    "object_size_parquet = [object_.size for object_ in boto3.resource('s3').Bucket(s3_parquet_bucket_name).objects.all()]\n",
    "\n",
    "print('Total size of csv backup: {:.1f} MB'.format(sum(object_size_csv) / (1024 ** 2)))\n",
    "print('Total size of parquet backup: {:.1f} MB'.format(sum(object_size_parquet) / (1024 ** 2)))\n",
    "\n",
    "print('Compression ratio: {:.2f}'.format(sum(object_size_csv) / sum(object_size_parquet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push data to PostgreSQL database <a id=database></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert records into the database\n",
    "\n",
    "dataframes = {\n",
    "    'calendar': df_calendar_process,\n",
    "    'host_verification': df_host_verifications_process,\n",
    "    'hosts': df_hosts_process,\n",
    "    'listing_amenities': df_amenities_process,\n",
    "    'listing_complements': df_listings_complements_process,\n",
    "    'listings': df_listings_main_process,\n",
    "    'neighbourhoods': df_neighbourhoods_process,\n",
    "    'reviews': df_reviews_process\n",
    "}\n",
    "\n",
    "for TABLE_NAME in dataframes.keys():\n",
    "    insert_to_db(dataframes[TABLE_NAME], TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "pyvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
