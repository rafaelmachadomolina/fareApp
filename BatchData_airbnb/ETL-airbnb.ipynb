{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL to transform Airbnb rawdata to database schema\n",
    "\n",
    "This ETL is coded in pyspark. Though the current data size could be handled in Pandas or similar libraries, pyspark was chosen for scale reasons.\n",
    "\n",
    "TODO: outline steps here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session:\n",
      "<pyspark.sql.session.SparkSession object at 0x7f8a841bf3a0>\n"
     ]
    }
   ],
   "source": [
    "### Import modules\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, Window, SQLContext\n",
    "from pyspark.sql import functions as psF\n",
    "from pyspark.sql import types as psDT\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkWithPostgres')\\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/project/Practical_exam/postgresql-42.3.2.jar\")\\\n",
    "        .getOrCreate()\n",
    "print('Spark session:')\n",
    "print(spark)\n",
    "\n",
    "import pyspark\n",
    "# from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation of Spark context\n",
      "<SparkContext master=local[*] appName=SparkWithPostgres>\n",
      "\n",
      "Initialisation of SparkSQL\n",
      "<pyspark.sql.context.SQLContext object at 0x7f8aa8c759a0>\n"
     ]
    }
   ],
   "source": [
    "### Initialise Spark context\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    "print('Initialisation of Spark context')\n",
    "print(sc)\n",
    "print('')\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "print('Initialisation of SparkSQL')\n",
    "print(sqlContext)\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants\n",
    "\n",
    "CSV_PATH = './OriginalData_csv/'\n",
    "PARQUET_PATH = './SchemaReadyData_parquet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of dataframe neighbourhoods\n",
      "root\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Load neighbourhoods to Spark dataframe\n",
    "\n",
    "df_neighbourhoods = spark.read.options(delimiter = ',',\n",
    "                                 header = True,\n",
    "                                 #lineSep = '\\n',\n",
    "                                 escape = '\"',\n",
    "                                 multiline = True).csv(CSV_PATH + 'neighbourhoods.csv')\n",
    "\n",
    "# Print schema\n",
    "print('Schema of dataframe neighbourhoods')\n",
    "print(df_neighbourhoods.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of table neighbourhood\n",
      "root\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Drop empty column and add index column\n",
    "\n",
    "df_neighbourhoods = df_neighbourhoods.drop('neighbourhood_group')\n",
    "\n",
    "# Trim neighbourhood column\n",
    "df_neighbourhoods = df_neighbourhoods.withColumn('neighbourhood', psF.trim('neighbourhood'))\n",
    "\n",
    "# Add index column\n",
    "df_neighbourhoods = df_neighbourhoods.withColumn('id',\n",
    "                                                 psF.row_number().over(Window.orderBy(psF.monotonically_increasing_id())))\n",
    "df_neighbourhoods = df_neighbourhoods.withColumn('id',\n",
    "                                                 df_neighbourhoods['id'].cast(psDT.LongType()))\n",
    "\n",
    "# Print schema\n",
    "print('Schema of table neighbourhood')\n",
    "print(df_neighbourhoods.printSchema())\n",
    "# df_neighbourhoods.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export neighbourhoods to parquet\n",
    "\n",
    "df_neighbourhoods.write.mode('overwrite').parquet(PARQUET_PATH + 'neighbourhoods.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of dataframe listings\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- scrape_id: string (nullable = true)\n",
      " |-- last_scraped: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- neighborhood_overview: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- host_url: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- host_since: string (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_about: string (nullable = true)\n",
      " |-- host_response_time: string (nullable = true)\n",
      " |-- host_response_rate: string (nullable = true)\n",
      " |-- host_acceptance_rate: string (nullable = true)\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- host_thumbnail_url: string (nullable = true)\n",
      " |-- host_picture_url: string (nullable = true)\n",
      " |-- host_neighbourhood: string (nullable = true)\n",
      " |-- host_listings_count: string (nullable = true)\n",
      " |-- host_total_listings_count: string (nullable = true)\n",
      " |-- host_verifications: string (nullable = true)\n",
      " |-- host_has_profile_pic: string (nullable = true)\n",
      " |-- host_identity_verified: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- neighbourhood_group_cleansed: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: string (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- bathrooms_text: string (nullable = true)\n",
      " |-- bedrooms: string (nullable = true)\n",
      " |-- beds: string (nullable = true)\n",
      " |-- amenities: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- maximum_nights: string (nullable = true)\n",
      " |-- minimum_minimum_nights: string (nullable = true)\n",
      " |-- maximum_minimum_nights: string (nullable = true)\n",
      " |-- minimum_maximum_nights: string (nullable = true)\n",
      " |-- maximum_maximum_nights: string (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: string (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: string (nullable = true)\n",
      " |-- calendar_updated: string (nullable = true)\n",
      " |-- has_availability: string (nullable = true)\n",
      " |-- availability_30: string (nullable = true)\n",
      " |-- availability_60: string (nullable = true)\n",
      " |-- availability_90: string (nullable = true)\n",
      " |-- availability_365: string (nullable = true)\n",
      " |-- calendar_last_scraped: string (nullable = true)\n",
      " |-- number_of_reviews: string (nullable = true)\n",
      " |-- number_of_reviews_ltm: string (nullable = true)\n",
      " |-- number_of_reviews_l30d: string (nullable = true)\n",
      " |-- first_review: string (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- review_scores_rating: string (nullable = true)\n",
      " |-- review_scores_accuracy: string (nullable = true)\n",
      " |-- review_scores_cleanliness: string (nullable = true)\n",
      " |-- review_scores_checkin: string (nullable = true)\n",
      " |-- review_scores_communication: string (nullable = true)\n",
      " |-- review_scores_location: string (nullable = true)\n",
      " |-- review_scores_value: string (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- calculated_host_listings_count: string (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: string (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: string (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: string (nullable = true)\n",
      " |-- reviews_per_month: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Load listings to Spark dataframe\n",
    "\n",
    "df_listings = spark.read.options(delimiter = ',',\n",
    "                                 header = True,\n",
    "                                 #lineSep = '\\n',\n",
    "                                 escape = '\"',\n",
    "                                 multiline = True).csv(CSV_PATH + 'listings.csv')\n",
    "\n",
    "# Print schema\n",
    "print('Schema of dataframe listings')\n",
    "print(df_listings.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop empty columns (for more insight, consult EDA notebook)\n",
    "\n",
    "COLUMNS_TO_DROP = ('neighbourhood_group_cleansed',\n",
    "                  'bathrooms',\n",
    "                  'calendar_updated',\n",
    "                  'license')\n",
    "\n",
    "df_listings = df_listings.drop(*COLUMNS_TO_DROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns on original file: 70\n",
      "Columns after the split: 70\n"
     ]
    }
   ],
   "source": [
    "### Create 4 tables from listings df\n",
    "\n",
    "print('Columns on original file:', len(df_listings.columns))\n",
    "\n",
    "COLS_LISTINGS = ['id',\n",
    "                 'host_id',\n",
    "                 'listing_url',\n",
    "                 'scrape_id',\n",
    "                 'last_scraped',\n",
    "                 'name',\n",
    "                 'description',\n",
    "                 'neighborhood_overview',\n",
    "                 'picture_url',\n",
    "                 'neighbourhood',\n",
    "                 'neighbourhood_cleansed',\n",
    "                 'latitude',\n",
    "                 'longitude',\n",
    "                 'property_type',\n",
    "                 'room_type',\n",
    "                 'accommodates',\n",
    "                 'bathrooms_text',\n",
    "                 'bedrooms',\n",
    "                 'beds',\n",
    "                 'price',\n",
    "                 'minimum_nights',\n",
    "                 'maximum_nights']\n",
    "\n",
    "COLS_LISTINGS_AMENITIES = ['id',\n",
    "                           'amenities']\n",
    "\n",
    "COLS_HOSTS = ['host_id',\n",
    "              'host_url',\n",
    "              'host_name',\n",
    "              'host_since',\n",
    "              'host_location',\n",
    "              'host_about',\n",
    "              'host_response_time',\n",
    "              'host_response_rate',\n",
    "              'host_acceptance_rate',\n",
    "              'host_is_superhost',\n",
    "              'host_thumbnail_url',\n",
    "              'host_picture_url',\n",
    "              'host_neighbourhood',\n",
    "              'host_listings_count',\n",
    "              'host_total_listings_count',\n",
    "              'host_verifications',\n",
    "              'host_has_profile_pic',\n",
    "              'host_identity_verified']\n",
    "\n",
    "COLS_COMPLENET = ['id',\n",
    "                  'minimum_minimum_nights',\n",
    "                  'maximum_minimum_nights',\n",
    "                  'minimum_maximum_nights',\n",
    "                  'maximum_maximum_nights',\n",
    "                  'minimum_nights_avg_ntm',\n",
    "                  'maximum_nights_avg_ntm',\n",
    "                  'has_availability',\n",
    "                  'availability_30',\n",
    "                  'availability_60',\n",
    "                  'availability_90',\n",
    "                  'availability_365',\n",
    "                  'calendar_last_scraped',\n",
    "                  'number_of_reviews',\n",
    "                  'number_of_reviews_ltm',\n",
    "                  'number_of_reviews_l30d',\n",
    "                  'first_review',\n",
    "                  'last_review',\n",
    "                  'review_scores_rating',\n",
    "                  'review_scores_accuracy',\n",
    "                  'review_scores_cleanliness',\n",
    "                  'review_scores_checkin',\n",
    "                  'review_scores_communication',\n",
    "                  'review_scores_location',\n",
    "                  'review_scores_value',\n",
    "                  'instant_bookable',\n",
    "                  'calculated_host_listings_count',\n",
    "                  'calculated_host_listings_count_entire_homes',\n",
    "                  'calculated_host_listings_count_private_rooms',\n",
    "                  'calculated_host_listings_count_shared_rooms',\n",
    "                  'reviews_per_month']\n",
    "\n",
    "print('Columns after the split:',\n",
    "      len(COLS_LISTINGS) + len(COLS_HOSTS) + len(COLS_COMPLENET) + len(COLS_LISTINGS_AMENITIES) - 3) #Correction for 3 additioanl id columns\n",
    "\n",
    "# Create dataframes\n",
    "df_listings_main = df_listings.select(COLS_LISTINGS)\n",
    "df_listings_amenities = df_listings.select(COLS_LISTINGS_AMENITIES)\n",
    "df_hosts = df_listings.select(COLS_HOSTS)\n",
    "df_listings_complements = df_listings.select(COLS_COMPLENET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process listings main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge with neighbourhoods to obtain neighbourhood_id\n",
    "\n",
    "df_neighbourhoods_merge = df_neighbourhoods.withColumnRenamed('id', 'neighbourhood_id') \\\n",
    "    .withColumnRenamed('neighbourhood', 'neighbourhood_cleansed')\n",
    "df_listings_main = df_listings_main.join(df_neighbourhoods_merge,\n",
    "                                         how = 'left',\n",
    "                                         on = ['neighbourhood_cleansed'])\n",
    "\n",
    "df_listings_main = df_listings_main.drop('neighbourhood_cleansed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of main listings\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- scrape_id: string (nullable = true)\n",
      " |-- last_scraped: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- neighborhood_overview: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: string (nullable = true)\n",
      " |-- bathrooms_text: string (nullable = true)\n",
      " |-- bedrooms: string (nullable = true)\n",
      " |-- beds: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- maximum_nights: string (nullable = true)\n",
      " |-- neighbourhood_id: long (nullable = true)\n",
      "\n",
      "None\n",
      "Schema of table listings\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- host_id: long (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- scrape_id: long (nullable = true)\n",
      " |-- date_last_scraped: date (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- neighborhood_overview: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- neighbourhood_typed: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: integer (nullable = true)\n",
      " |-- bathrooms_text: string (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      " |-- neighbourhood_id: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Give structure to listings table\n",
    "\n",
    "print('Schema of main listings')\n",
    "print(df_listings_main.printSchema())\n",
    "\n",
    "# Define dict with datatypes\n",
    "LISTINGS_MAIN_DTYPES = {'id': psDT.LongType(),\n",
    "                        'host_id': psDT.LongType(),\n",
    "                        'scrape_id': psDT.LongType(),\n",
    "                        'last_scraped': psDT.DateType(),\n",
    "                        'latitude': psDT.DoubleType(),\n",
    "                        'longitude': psDT.DoubleType(),\n",
    "                        'accommodates': psDT.IntegerType(),\n",
    "                        'bedrooms': psDT.IntegerType(),\n",
    "                        'beds': psDT.IntegerType(),\n",
    "                        'price': psDT.DoubleType(),\n",
    "                        'minimum_nights': psDT.IntegerType(),\n",
    "                        'maximum_nights': psDT.IntegerType(),\n",
    "                        'neighbourhood_id': psDT.IntegerType()}\n",
    "\n",
    "# Define dict with new names\n",
    "LISTINGS_MAIN_NAMES = {'last_scraped': 'date_last_scraped',\n",
    "                      'neighbourhood': 'neighbourhood_typed'}\n",
    "\n",
    "# Copy dataframe\n",
    "df_listings_main_process = df_listings_main.select('*')\n",
    "\n",
    "# Drop $ sign from price\n",
    "df_listings_main_process = df_listings_main_process.withColumn('price',\n",
    "                                                               psF.regexp_replace('price', '\\$', ''))\n",
    "\n",
    "# Use stripping for strings\n",
    "for COLUMN in df_listings_main.columns:\n",
    "    \n",
    "    # Strip whitespaces\n",
    "    df_listings_main_process = df_listings_main_process.withColumn(COLUMN, psF.trim(COLUMN))\n",
    "    \n",
    "    # Replace None and N/A with null\n",
    "    df_listings_main_process = df_listings_main_process.withColumn(COLUMN,\n",
    "                                                                   psF.when((psF.col(COLUMN) == 'None') | (psF.col(COLUMN) == 'N/A'), None) \\\n",
    "                                                                   .otherwise(psF.col(COLUMN)))\n",
    "    \n",
    "    # Change data type\n",
    "    if(COLUMN in LISTINGS_MAIN_DTYPES.keys()):\n",
    "        df_listings_main_process = df_listings_main_process.withColumn(COLUMN,\n",
    "                                                                       df_listings_main_process[COLUMN] \\\n",
    "                                                                       .cast(LISTINGS_MAIN_DTYPES[COLUMN]))\n",
    "    \n",
    "    # Rename columns\n",
    "    if(COLUMN in LISTINGS_MAIN_NAMES.keys()):\n",
    "        df_listings_main_process = df_listings_main_process.withColumnRenamed(COLUMN,\n",
    "                                                                             LISTINGS_MAIN_NAMES[COLUMN])\n",
    "        \n",
    "# Print schema\n",
    "print('Schema of table listings')\n",
    "print(df_listings_main_process.printSchema())\n",
    "# df_listings_main_process.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export listings to parquet\n",
    "\n",
    "df_listings_main_process.write.mode('overwrite').parquet(PARQUET_PATH + 'listings.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process hosts - main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows on previous dataframe: 66641\n",
      "Rows on unique dataframe: 44695\n"
     ]
    }
   ],
   "source": [
    "### Drop duplicates\n",
    "\n",
    "df_hosts_unique = df_hosts.dropDuplicates()\n",
    "\n",
    "print('Rows on previous dataframe:', df_hosts.count())\n",
    "print('Rows on unique dataframe:', df_hosts_unique.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Separate host verifications\n",
    "\n",
    "df_host_verifications = df_hosts_unique.select(['host_id', 'host_verifications'])\n",
    "df_hosts_sliced = df_hosts_unique.drop('host_verifications')\n",
    "\n",
    "# Replace None with null and drop those records (it means host has no verifications at all)\n",
    "df_host_verifications = df_host_verifications.withColumn('host_verifications',\n",
    "                                                        psF.when(psF.col('host_verifications') == 'None', None) \\\n",
    "                                                         .otherwise(psF.col('host_verifications')))\n",
    "df_host_verifications = df_host_verifications.na.drop(subset = ['host_verifications'])\n",
    "# df_host_verifications.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of hosts sliced\n",
      "root\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- host_url: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- host_since: string (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_about: string (nullable = true)\n",
      " |-- host_response_time: string (nullable = true)\n",
      " |-- host_response_rate: string (nullable = true)\n",
      " |-- host_acceptance_rate: string (nullable = true)\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- host_thumbnail_url: string (nullable = true)\n",
      " |-- host_picture_url: string (nullable = true)\n",
      " |-- host_neighbourhood: string (nullable = true)\n",
      " |-- host_listings_count: string (nullable = true)\n",
      " |-- host_total_listings_count: string (nullable = true)\n",
      " |-- host_has_profile_pic: string (nullable = true)\n",
      " |-- host_identity_verified: string (nullable = true)\n",
      "\n",
      "None\n",
      "Schema of table hosts\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- since: date (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- about: string (nullable = true)\n",
      " |-- response_time: string (nullable = true)\n",
      " |-- response_rate: double (nullable = true)\n",
      " |-- acceptance_rate: double (nullable = true)\n",
      " |-- is_superhost: boolean (nullable = true)\n",
      " |-- thumbnail_url: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- listings_count: integer (nullable = true)\n",
      " |-- total_listings_count: integer (nullable = true)\n",
      " |-- has_profile_pic: boolean (nullable = true)\n",
      " |-- identity_verified: boolean (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Give structure to hosts sliced table\n",
    "\n",
    "print('Schema of hosts sliced')\n",
    "print(df_hosts_sliced.printSchema())\n",
    "\n",
    "# Define dict with datatypes\n",
    "HOST_DTYPES = {'host_id': psDT.LongType(),\n",
    "               'host_since': psDT.DateType(),\n",
    "               'host_response_rate': psDT.DoubleType(),\n",
    "               'host_acceptance_rate': psDT.DoubleType(),\n",
    "               'host_is_superhost': psDT.BooleanType(),\n",
    "               'host_listings_count': psDT.IntegerType(),\n",
    "               'host_total_listings_count': psDT.IntegerType(),\n",
    "               'host_has_profile_pic': psDT.BooleanType(),\n",
    "               'host_identity_verified': psDT.BooleanType()}\n",
    "\n",
    "# Define features to be converted to bool\n",
    "HOST_BOOL = [i for i in HOST_DTYPES.keys() if HOST_DTYPES[i] == psDT.BooleanType()]\n",
    "\n",
    "# Define features to be transformed to decimal from percentage\n",
    "PERCENTAGE_FEAT = ['host_response_rate', 'host_acceptance_rate']\n",
    "\n",
    "# Copy dataframe\n",
    "df_hosts_process = df_hosts_sliced.select('*')\n",
    "\n",
    "# Drop % sign from percentage feats\n",
    "for COLUMN in PERCENTAGE_FEAT:\n",
    "    df_hosts_process = df_hosts_process.withColumn(COLUMN,\n",
    "                                                   psF.regexp_replace(COLUMN, '\\%', ''))\n",
    "\n",
    "# Use stripping for strings\n",
    "for COLUMN in df_hosts_process.columns:\n",
    "    \n",
    "    # Strip whitespaces\n",
    "    df_hosts_process = df_hosts_process.withColumn(COLUMN, psF.trim(COLUMN))\n",
    "    \n",
    "    # Replace None and N/A with null\n",
    "    df_hosts_process = df_hosts_process.withColumn(COLUMN,\n",
    "                                                   psF.when((psF.col(COLUMN) == 'None') | (psF.col(COLUMN) == 'N/A'), None) \\\n",
    "                                                   .otherwise(psF.col(COLUMN)))\n",
    "    \n",
    "    # Change data type\n",
    "    if(COLUMN in HOST_DTYPES.keys()):\n",
    "        df_hosts_process = df_hosts_process.withColumn(COLUMN,\n",
    "                                                       df_hosts_process[COLUMN] \\\n",
    "                                                       .cast(HOST_DTYPES[COLUMN]))\n",
    "        \n",
    "    # Make percentage columns decimal\n",
    "    if(COLUMN in PERCENTAGE_FEAT):\n",
    "        df_hosts_process = df_hosts_process.withColumn(COLUMN,\n",
    "                                                   psF.col(COLUMN) / 100)\n",
    "    \n",
    "    # Rename columns by removing host_\n",
    "    df_hosts_process = df_hosts_process.withColumnRenamed(COLUMN, COLUMN[5:])\n",
    "        \n",
    "# Print schema\n",
    "print('Schema of table hosts')\n",
    "print(df_hosts_process.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export hosts to parquet\n",
    "\n",
    "df_hosts_process.write.mode('overwrite').parquet(PARQUET_PATH + 'hosts.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process hosts - verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate function to parse array of strings to rows containing array elements\n",
    "### Credit: https://silpara.medium.com/pyspark-string-to-array-of-string-in-dataframe-b9572233ccea\n",
    "def parse_array_from_string(x):\n",
    "    res = json.loads(x)\n",
    "    return res\n",
    "\n",
    "retrieve_array_func = psF.udf(parse_array_from_string, psDT.ArrayType(psDT.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of hosts verifications\n",
      "root\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- host_verifications: string (nullable = true)\n",
      "\n",
      "None\n",
      "Schema of table host verification\n",
      "root\n",
      " |-- host_id: long (nullable = true)\n",
      " |-- verification: string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Explode verifications into different rows for final table\n",
    "\n",
    "print('Schema of hosts verifications')\n",
    "print(df_host_verifications.printSchema())\n",
    "\n",
    "# Copy dataframe\n",
    "df_host_verifications_process = df_host_verifications.select('*')\n",
    "\n",
    "# Convert quotation to double quotation --> in order to properly load to json\n",
    "df_host_verifications_process = df_host_verifications_process.withColumn('host_verifications',\n",
    "                                                   psF.regexp_replace('host_verifications', '\\'', '\\\"'))\n",
    "\n",
    "# Change data types\n",
    "df_host_verifications_process = df_host_verifications_process.withColumn('host_id',\n",
    "                                                       df_host_verifications_process['host_id'].cast(psDT.LongType()))\n",
    "df_host_verifications_process = df_host_verifications_process.withColumn('host_verifications',\n",
    "                                                       retrieve_array_func(psF.col('host_verifications')))\n",
    "\n",
    "# Explode list into rows\n",
    "df_host_verifications_process = df_host_verifications_process.select('host_id',\n",
    "                                                                    psF.explode(df_host_verifications_process.host_verifications) \\\n",
    "                                                                    .alias('verification'))\n",
    "\n",
    "# Add index column\n",
    "df_host_verifications_process = df_host_verifications_process.withColumn('id',\n",
    "                                                                         psF.row_number() \\\n",
    "                                                                         .over(Window \\\n",
    "                                                                               .orderBy(psF.monotonically_increasing_id())))\n",
    "df_host_verifications_process = df_host_verifications_process.withColumn('id',\n",
    "                                                       df_host_verifications_process['id'].cast(psDT.LongType()))\n",
    "\n",
    "# Trim values of string\n",
    "df_host_verifications_process = df_host_verifications_process.withColumn('verification', psF.trim('verification'))\n",
    "\n",
    "# Print schema\n",
    "print('Schema of table host verification')\n",
    "print(df_host_verifications_process.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export host verification to parquet\n",
    "\n",
    "df_host_verifications_process.write.mode('overwrite').parquet(PARQUET_PATH + 'host_verification.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process listings amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of amenities\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- amenities: string (nullable = true)\n",
      "\n",
      "None\n",
      "Schema of table listing amenities\n",
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- amenity: string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n",
      "None\n",
      "+----------+--------------------+---+\n",
      "|listing_id|             amenity| id|\n",
      "+----------+--------------------+---+\n",
      "|     13913|           Hot water|  1|\n",
      "|     13913|             Heating|  2|\n",
      "|     13913|        Coffee maker|  3|\n",
      "|     13913|      Building staff|  4|\n",
      "|     13913|TV with standard ...|  5|\n",
      "|     13913|Pack ’n play/Trav...|  6|\n",
      "|     13913|Children’s books ...|  7|\n",
      "|     13913|   Fire extinguisher|  8|\n",
      "|     13913|                Iron|  9|\n",
      "|     13913|Free parking on p...| 10|\n",
      "|     13913|Lock on bedroom door| 11|\n",
      "|     13913|Luggage dropoff a...| 12|\n",
      "|     13913|               Dryer| 13|\n",
      "|     13913|          Hair dryer| 14|\n",
      "|     13913|Room-darkening sh...| 15|\n",
      "|     13913|               Stove| 16|\n",
      "|     13913|                Oven| 17|\n",
      "|     13913|Babysitter recomm...| 18|\n",
      "|     13913|                Wifi| 19|\n",
      "|     13913|      Cooking basics| 20|\n",
      "+----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Explode verifications into different rows for final table\n",
    "\n",
    "print('Schema of amenities')\n",
    "print(df_listings_amenities.printSchema())\n",
    "\n",
    "# Copy dataframe\n",
    "df_amenities_process = df_listings_amenities.select('*')\n",
    "\n",
    "# Rename id columns to listing_id\n",
    "df_amenities_process = df_amenities_process.withColumnRenamed('id', 'listing_id')\n",
    "\n",
    "# Change data types\n",
    "df_amenities_process = df_amenities_process.withColumn('listing_id',\n",
    "                                                       df_amenities_process['listing_id'].cast(psDT.LongType()))\n",
    "df_amenities_process = df_amenities_process.withColumn('amenities',\n",
    "                                                       retrieve_array_func(psF.col('amenities')))\n",
    "\n",
    "# Explode list into rows\n",
    "df_amenities_process = df_amenities_process.select('listing_id',\n",
    "                                                   psF.explode(df_amenities_process.amenities) \\\n",
    "                                                   .alias('amenity'))\n",
    "\n",
    "# Add index column\n",
    "df_amenities_process = df_amenities_process.withColumn('id',\n",
    "                                                       psF.row_number() \\\n",
    "                                                       .over(Window \\\n",
    "                                                             .orderBy(psF.monotonically_increasing_id())))\n",
    "df_amenities_process = df_amenities_process.withColumn('id',\n",
    "                                                       df_amenities_process['id'].cast(psDT.LongType()))\n",
    "\n",
    "# Trim values of string\n",
    "df_amenities_process = df_amenities_process.withColumn('amenity', psF.trim('amenity'))\n",
    "\n",
    "# Print schema\n",
    "print('Schema of table listing amenities')\n",
    "print(df_amenities_process.printSchema())\n",
    "df_amenities_process.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export listing amenities to parquet\n",
    "\n",
    "df_amenities_process.write.mode('overwrite').parquet(PARQUET_PATH + 'listing_amenities.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process listings complements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of listings complements\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- minimum_minimum_nights: string (nullable = true)\n",
      " |-- maximum_minimum_nights: string (nullable = true)\n",
      " |-- minimum_maximum_nights: string (nullable = true)\n",
      " |-- maximum_maximum_nights: string (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: string (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: string (nullable = true)\n",
      " |-- has_availability: string (nullable = true)\n",
      " |-- availability_30: string (nullable = true)\n",
      " |-- availability_60: string (nullable = true)\n",
      " |-- availability_90: string (nullable = true)\n",
      " |-- availability_365: string (nullable = true)\n",
      " |-- calendar_last_scraped: string (nullable = true)\n",
      " |-- number_of_reviews: string (nullable = true)\n",
      " |-- number_of_reviews_ltm: string (nullable = true)\n",
      " |-- number_of_reviews_l30d: string (nullable = true)\n",
      " |-- first_review: string (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- review_scores_rating: string (nullable = true)\n",
      " |-- review_scores_accuracy: string (nullable = true)\n",
      " |-- review_scores_cleanliness: string (nullable = true)\n",
      " |-- review_scores_checkin: string (nullable = true)\n",
      " |-- review_scores_communication: string (nullable = true)\n",
      " |-- review_scores_location: string (nullable = true)\n",
      " |-- review_scores_value: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- calculated_host_listings_count: string (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: string (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: string (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: string (nullable = true)\n",
      " |-- reviews_per_month: string (nullable = true)\n",
      "\n",
      "None\n",
      "Schema of table listings complements\n",
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- minimum_minimum_nights: integer (nullable = true)\n",
      " |-- maximum_minimum_nights: integer (nullable = true)\n",
      " |-- minimum_maximum_nights: integer (nullable = true)\n",
      " |-- maximum_maximum_nights: integer (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: double (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: double (nullable = true)\n",
      " |-- has_availability: boolean (nullable = true)\n",
      " |-- availability_30: integer (nullable = true)\n",
      " |-- availability_60: integer (nullable = true)\n",
      " |-- availability_90: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- calendar_last_scraped: date (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- number_of_reviews_l30d: integer (nullable = true)\n",
      " |-- first_review: date (nullable = true)\n",
      " |-- last_review: date (nullable = true)\n",
      " |-- review_scores_rating: double (nullable = true)\n",
      " |-- review_scores_accuracy: double (nullable = true)\n",
      " |-- review_scores_cleanliness: double (nullable = true)\n",
      " |-- review_scores_checkin: double (nullable = true)\n",
      " |-- review_scores_communication: double (nullable = true)\n",
      " |-- review_scores_location: double (nullable = true)\n",
      " |-- review_scores_value: double (nullable = true)\n",
      " |-- instant_bookable: boolean (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: integer (nullable = true)\n",
      " |-- reviews_per_month: double (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Give structure to listings table\n",
    "\n",
    "print('Schema of listings complements')\n",
    "print(df_listings_complements.printSchema())\n",
    "\n",
    "# Define dict with datatypes\n",
    "COMPLEMENTS_MAIN_DTPYES = {'listing_id': psDT.LongType(),\n",
    "                          'minimum_minimum_nights': psDT.IntegerType(),\n",
    "                          'maximum_minimum_nights': psDT.IntegerType(),\n",
    "                          'minimum_maximum_nights': psDT.IntegerType(),\n",
    "                          'maximum_maximum_nights': psDT.IntegerType(),\n",
    "                          'minimum_nights_avg_ntm': psDT.DoubleType(),\n",
    "                          'maximum_nights_avg_ntm': psDT.DoubleType(),\n",
    "                          'has_availability': psDT.BooleanType(),\n",
    "                          'availability_30': psDT.IntegerType(),\n",
    "                          'availability_60': psDT.IntegerType(),\n",
    "                          'availability_90': psDT.IntegerType(),\n",
    "                          'availability_365': psDT.IntegerType(),\n",
    "                          'calendar_last_scraped': psDT.DateType(),\n",
    "                          'number_of_reviews': psDT.IntegerType(),\n",
    "                          'number_of_reviews_ltm': psDT.IntegerType(),\n",
    "                          'number_of_reviews_l30d': psDT.IntegerType(),\n",
    "                          'first_review': psDT.DateType(),\n",
    "                          'last_review': psDT.DateType(),\n",
    "                          'review_scores_rating': psDT.DoubleType(),\n",
    "                          'review_scores_accuracy': psDT.DoubleType(),\n",
    "                          'review_scores_cleanliness': psDT.DoubleType(),\n",
    "                          'review_scores_checkin': psDT.DoubleType(),\n",
    "                          'review_scores_communication': psDT.DoubleType(),\n",
    "                          'review_scores_location': psDT.DoubleType(),\n",
    "                          'review_scores_value': psDT.DoubleType(),\n",
    "                          'instant_bookable': psDT.BooleanType(),\n",
    "                          'calculated_host_listings_count': psDT.IntegerType(),\n",
    "                          'calculated_host_listings_count_entire_homes': psDT.IntegerType(),\n",
    "                          'calculated_host_listings_count_private_rooms': psDT.IntegerType(),\n",
    "                          'calculated_host_listings_count_shared_rooms': psDT.IntegerType(),\n",
    "                          'reviews_per_month': psDT.DoubleType()}\n",
    "\n",
    "# Copy dataframe\n",
    "df_listings_complements_process = df_listings_complements.select('*')\n",
    "\n",
    "# Rename id columns to listing_id\n",
    "df_listings_complements_process = df_listings_complements_process.withColumnRenamed('id', 'listing_id')\n",
    "\n",
    "# Transformations per column\n",
    "for COLUMN in df_listings_complements_process.columns:\n",
    "    \n",
    "    # Strip whitespaces\n",
    "    df_listings_complements_process = df_listings_complements_process.withColumn(COLUMN, psF.trim(COLUMN))\n",
    "    \n",
    "    # Replace None and N/A with null\n",
    "    df_listings_complements_process = df_listings_complements_process.withColumn(COLUMN,\n",
    "                                                                                 psF.when((psF.col(COLUMN) == 'None') | (psF.col(COLUMN) == 'N/A'), None) \\\n",
    "                                                                                 .otherwise(psF.col(COLUMN)))\n",
    "    \n",
    "    # Change data type\n",
    "    if(COLUMN in COMPLEMENTS_MAIN_DTPYES.keys()):\n",
    "        df_listings_complements_process = df_listings_complements_process.withColumn(COLUMN,\n",
    "                                                                                     df_listings_complements_process[COLUMN] \\\n",
    "                                                                                     .cast(COMPLEMENTS_MAIN_DTPYES[COLUMN]))\n",
    "\n",
    "# Add index column\n",
    "df_listings_complements_process = df_listings_complements_process.withColumn('id',\n",
    "                                                                             psF.row_number() \\\n",
    "                                                                             .over(Window \\\n",
    "                                                                                   .orderBy(psF.monotonically_increasing_id())))\n",
    "df_listings_complements_process = df_listings_complements_process.withColumn('id',\n",
    "                                                                             df_listings_complements_process['id'].cast(psDT.LongType()))\n",
    "        \n",
    "# Print schema\n",
    "print('Schema of table listings complements')\n",
    "print(df_listings_complements_process.printSchema())\n",
    "# df_listings_complements_process.select('reviews_per_month').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export listing complements to parquet\n",
    "\n",
    "df_listings_complements_process.write.mode('overwrite').parquet(PARQUET_PATH + 'listing_complements.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of dataframe reviews\n",
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- reviewer_id: string (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Load reviews to Spark dataframe\n",
    "\n",
    "df_reviews = spark.read.options(delimiter = ',',\n",
    "                                 header = True,\n",
    "                                 #lineSep = '\\n',\n",
    "                                 escape = '\"',\n",
    "                                 multiline = True).csv(CSV_PATH + 'reviews.csv')\n",
    "\n",
    "# Print schema\n",
    "print('Schema of dataframe reviews')\n",
    "print(df_reviews.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of reviews\n",
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- reviewer_id: string (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n",
      "None\n",
      "Schema of table listings complements\n",
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- reviewer_id: long (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Give structure to reviews table\n",
    "\n",
    "print('Schema of reviews')\n",
    "print(df_reviews.printSchema())\n",
    "\n",
    "# Define dict with datatypes\n",
    "REVIEWS_MAIN_DTPYES = {'listing_id': psDT.LongType(),\n",
    "                      'id': psDT.LongType(),\n",
    "                      'date': psDT.DateType(),\n",
    "                      'reviewer_id': psDT.LongType()}\n",
    "\n",
    "# Copy dataframe\n",
    "df_reviews_process = df_reviews.select('*')\n",
    "\n",
    "# Transformations per column\n",
    "for COLUMN in df_reviews_process.columns:\n",
    "    \n",
    "    # Strip whitespaces\n",
    "    df_reviews_process = df_reviews_process.withColumn(COLUMN, psF.trim(COLUMN))\n",
    "    \n",
    "    # Replace None and N/A with null\n",
    "    df_reviews_process = df_reviews_process.withColumn(COLUMN,\n",
    "                                                       psF.when((psF.col(COLUMN) == 'None') | (psF.col(COLUMN) == 'N/A'), None) \\\n",
    "                                                       .otherwise(psF.col(COLUMN)))\n",
    "    \n",
    "    # Change data type\n",
    "    if(COLUMN in REVIEWS_MAIN_DTPYES.keys()):\n",
    "        df_reviews_process = df_reviews_process.withColumn(COLUMN,\n",
    "                                                           df_reviews_process[COLUMN] \\\n",
    "                                                           .cast(REVIEWS_MAIN_DTPYES[COLUMN]))\n",
    "    \n",
    "# Print schema\n",
    "print('Schema of table listings complements')\n",
    "print(df_reviews_process.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export reviews to parquet\n",
    "\n",
    "df_reviews_process.write.mode('overwrite').parquet(PARQUET_PATH + 'reviews.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of dataframe calendar\n",
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- available: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- adjusted_price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- maximum_nights: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Load reviews to Spark dataframe\n",
    "\n",
    "df_calendar = spark.read.options(delimiter = ',',\n",
    "                                 header = True,\n",
    "                                 #lineSep = '\\n',\n",
    "                                 escape = '\"',\n",
    "                                 multiline = True).csv(CSV_PATH + 'calendar.csv')\n",
    "\n",
    "# Print schema\n",
    "print('Schema of dataframe calendar')\n",
    "print(df_calendar.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of calendar\n",
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- available: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- adjusted_price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- maximum_nights: string (nullable = true)\n",
      "\n",
      "None\n",
      "Schema of table calendar\n",
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- available: boolean (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- adjusted_price: double (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n",
      "None\n",
      "+----------+----------+---------+-----+--------------+--------------+--------------+---+\n",
      "|listing_id|      date|available|price|adjusted_price|minimum_nights|maximum_nights| id|\n",
      "+----------+----------+---------+-----+--------------+--------------+--------------+---+\n",
      "|    182802|2021-12-09|     true| 55.0|          55.0|             1|          1125|  1|\n",
      "|     13913|2021-12-09|    false| 65.0|          65.0|             1|            29|  2|\n",
      "|     13913|2021-12-10|    false| 65.0|          65.0|             1|            29|  3|\n",
      "|     13913|2021-12-11|    false| 65.0|          65.0|             1|            29|  4|\n",
      "|     13913|2021-12-12|     true| 65.0|          65.0|             1|            29|  5|\n",
      "+----------+----------+---------+-----+--------------+--------------+--------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Give structure to calendar\n",
    "\n",
    "print('Schema of calendar')\n",
    "print(df_calendar.printSchema())\n",
    "\n",
    "# Define dict with datatypes\n",
    "CALENDAR_MAIN_DTPYES = {'listing_id': psDT.LongType(),\n",
    "                       'date': psDT.DateType(),\n",
    "                       'available': psDT.BooleanType(),\n",
    "                       'price': psDT.DoubleType(),\n",
    "                       'adjusted_price': psDT.DoubleType(),\n",
    "                       'minimum_nights': psDT.IntegerType(),\n",
    "                       'maximum_nights': psDT.IntegerType()}\n",
    "\n",
    "# Copy dataframe\n",
    "df_calendar_process = df_calendar.select('*')\n",
    "\n",
    "# Drop $ sign from price and adjusted_price\n",
    "df_calendar_process = df_calendar_process.withColumn('price',\n",
    "                                                     psF.regexp_replace('price', '\\$', ''))\n",
    "df_calendar_process = df_calendar_process.withColumn('adjusted_price',\n",
    "                                                     psF.regexp_replace('adjusted_price', '\\$', ''))\n",
    "\n",
    "# # Transformations per column\n",
    "for COLUMN in df_calendar_process.columns:\n",
    "    \n",
    "    # Strip whitespaces\n",
    "    df_calendar_process = df_calendar_process.withColumn(COLUMN, psF.trim(COLUMN))\n",
    "    \n",
    "    # Replace None and N/A with null\n",
    "    df_calendar_process = df_calendar_process.withColumn(COLUMN,\n",
    "                                                         psF.when((psF.col(COLUMN) == 'None') | (psF.col(COLUMN) == 'N/A'), None) \\\n",
    "                                                         .otherwise(psF.col(COLUMN)))\n",
    "    \n",
    "    # Change data type\n",
    "    if(COLUMN in CALENDAR_MAIN_DTPYES.keys()):\n",
    "        df_calendar_process = df_calendar_process.withColumn(COLUMN,\n",
    "                                                             df_calendar_process[COLUMN] \\\n",
    "                                                             .cast(CALENDAR_MAIN_DTPYES[COLUMN]))\n",
    "\n",
    "# Add index column\n",
    "df_calendar_process = df_calendar_process.withColumn('id',\n",
    "                                                     psF.row_number() \\\n",
    "                                                     .over(Window \\\n",
    "                                                           .orderBy(psF.monotonically_increasing_id())))\n",
    "df_calendar_process = df_calendar_process.withColumn('id',\n",
    "                                                     df_calendar_process['id'].cast(psDT.LongType()))\n",
    "        \n",
    "# Print schema\n",
    "print('Schema of table calendar')\n",
    "print(df_calendar_process.printSchema())\n",
    "df_calendar_process.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export calendar to parquet\n",
    "\n",
    "df_calendar_process.write.mode('overwrite').parquet(PARQUET_PATH + 'calendar.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "pyvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
